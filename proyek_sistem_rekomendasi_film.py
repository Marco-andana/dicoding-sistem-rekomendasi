# -*- coding: utf-8 -*-
"""Proyek Sistem Rekomendasi Film.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wIr8PoIA1Xqd4-7KdEF19wOAoceAPcwG
"""

#melakukan import data dari file
from google.colab import drive
drive.mount('/content/drive')

"""# Import Library
Melakukan import pada library yang dibutuhkan
"""

#melakukan import library pandas dan memasukkan tiap file kedalam variabel dan membatasi hanya mengambil 100.000 baris
import pandas as pd

tagrelevances = pd.read_csv('/content/drive/MyDrive/Artificial_Intellegent/Data/Movielens_20m/genome_scores.csv', nrows=100000)
tagdetails = pd.read_csv('/content/drive/MyDrive/Artificial_Intellegent/Data/Movielens_20m/genome_tags.csv', nrows=100000)
externallinks = pd.read_csv('/content/drive/MyDrive/Artificial_Intellegent/Data/Movielens_20m/link.csv', nrows=100000)
movies = pd.read_csv('/content/drive/MyDrive/Artificial_Intellegent/Data/Movielens_20m/movie.csv', nrows=100000)
ratings = pd.read_csv('/content/drive/MyDrive/Artificial_Intellegent/Data/Movielens_20m/rating.csv', nrows=100000)
tags = pd.read_csv('/content/drive/MyDrive/Artificial_Intellegent/Data/Movielens_20m/tag.csv', nrows=100000)

print(tagrelevances.shape)
print(tagdetails.shape)
print(externallinks.shape)
print(movies.shape)
print(ratings.shape)
print(tags.shape)

#mengecek jumlah total movies dan tags
print('Jumlah total movies: ', len(movies.movieId.unique()))
print('Jumlah total tags: ', len(tags))

"""# Univariate Exploratory Data Analysis
Pada bagian ini saya melakukan pengeksplorasian data dengan menggunakan pengecekan pada file yang akan digunakan, lalu melakukan penggabungan atau concat.
"""

movies.info() #mengecek info pada file movie

print('Total genre (angka):', movies.genres.nunique())
print('Total genre yang ada: ', movies.genres.unique())

#mengecek total genre dalam bentuk angka dan dalam bentuk string

print(tagrelevances.shape) #mengecek shape dari relevansi tag

tagrelevances.head() #mengecek potongan data teratas tag

ratings.head()#mengecek potongan data teratas tag

ratings.describe()#mengecek informasi terkait file ratings

print('Jumlah userId: ', len(ratings.userId.unique()))
print('Jumlah movieId: ', len(ratings.movieId.unique()))
print('Jumlah data rating: ', len(ratings))

#mengecek jumlah user, movie, dan data rating

#Melakukan penggabungan untuk file movie berdasarkan movieId yang ada pada tiap file
print('Jumlah seluruh movie sebelum penggabungan: ', len(movies.movieId.unique()))

import numpy as np

movie_all = np.concatenate((
    movies.movieId.unique(),
    ratings.movieId.unique(),
    tagrelevances.movieId.unique(),
    externallinks.movieId.unique(),
    tags.movieId.unique()
))

movie_all = np.sort(np.unique(movie_all))

print('Jumlah seluruh data movies berdasarkan movieId: ', len(movie_all))

#Melakukan penggabungan untuk file ratings berdasarkan userid yang ada pada tiap file
print('Jumah seluruh userId sebelum penggabungan' ,len(ratings.userId.unique()))

user_all = np.concatenate((
    ratings.userId.unique(),
    tags.userId.unique()
))

user_all = np.sort(np.unique(user_all))

print('Jumlah seluruh data userId yang memberi rate berdasarkan userId' ,len(user_all))

"""# Data Preprocessing
Pada data preprocessing saya mengecek untuk null value pada variabel movie. Terdapat banyak sekali null value dan untuk mengatasinya saya mendrop table yang mengandung null value. Lalu saya melakukan merge pada tabel-tabel yang diperlukan untuk mempersiapkan dataset dengan lebih baik.

"""

#Pada bagian ini dilakukan merge untuk menyiapkan data
movie_info = pd.concat([movies, tags])

movie = pd.merge(ratings, movie_info, on='movieId', how='left')

movie

movie.isnull().sum() #mengecek apakah ada null value

movie.groupby('movieId').sum() # mengubah pengelompokkan data table menjadi berdasarkan movieId

all_movies_rate = ratings #memasukkan file ratings ke dalam variabel all_movies_rate
all_movies_rate

all_resto_name = pd.merge(all_movies_rate, movies[['movieId', 'title']], on='movieId', how='left') #melakukan merge pada file movies dan variabel all movies rate dengan hanya mengambil movieId dan title
all_resto_name

all_tags = pd.merge(tagrelevances, tagdetails, on='tagId', how='left') #merge kedua variabel
all_tags

#menyortir tags berdasarkan top 3 relevansi lalu melakukan merge
sorted_tags = all_tags.sort_values(['movieId', 'relevance'], ascending=[True, False])
top_tags = sorted_tags.groupby('movieId').head(3)

all_movie = pd.merge(all_resto_name, top_tags[['movieId', 'tag']], on='movieId', how='left')
all_movie

#merge lagi hingga jadi satu kolom
top_tags_aggregated = top_tags.groupby('movieId')['tag'].apply(', '.join).reset_index()
top_tags_aggregated.rename(columns={'tag': 'tags'}, inplace=True)

all_movie = pd.merge(all_resto_name, top_tags_aggregated, on='movieId', how='left')
all_movie = all_movie.drop(columns=['timestamp'])
all_movie

"""# Data Preparation
Pada bagian ini saya melakukan persiapan data dengan melakukan nilai null kembali pada tabel all_movie. Terdapat missing value atau null pada kolom tags yang dimana saya melakukan drop atau penghapusan pada tabel yang nilai null.

Setelah itu saya melakukan pengecekan untuk tags yang ambigu dalam makna dan melakukan penghapusan pada kumpulan tags ambigu tersebut.

"""

all_movie.isnull().sum() #mengecek nilai null

all_movie_clean = all_movie.dropna() #mendrop tabel yang memiliki nilai null
all_movie_clean

all_movie_clean.isnull().sum() #melakukan pengecekan ulang terkait nilai null

#melakukan sort berdasarkan movieId
fix_movies = all_movie_clean.sort_values('movieId', ascending=True)
fix_movies

len(fix_movies.movieId.unique()) #mengecek panjang movie yang unik berdasarkan movieId

fix_movies.tags.unique() #mengecek nama-nama tags pada movie

fix_movies[fix_movies['tags'].str.contains('cute!', na=False)] #mengecek baris mana saja yang terdapat tags cute

fix_movies[fix_movies['tags'].str.contains('stupidity', na=False)] #mengecek baris mana saja yang terdapat tags stupidity

fix_movies[fix_movies['tags'].str.contains('mentor', na=False)] #mengecek baris mana saja yang terdapat tags mentor

#melakukan penghapusan pada tags yang memiliki makna ambigu
import re

tags_to_remove = ["cute!", "stupid", "mentor", "sequels"]
pattern = r'\b(' + '|'.join(map(re.escape, tags_to_remove)) + r')\b,?\s*'

fix_movies['tags'] = fix_movies['tags'].str.replace(pattern, '', regex=True)

fix_movies[fix_movies['tags'].str.contains('mentor', na=False)] #mengecek ulang apakah penghapusan berhasil

"""## Preparation
Saya melakukan persiapan lanjutan dimana dimulai dengan sortir, dan melakukan penghapusan pada data duplikat seperti pada code cell ke 2. Mengubah data series menjadi bentuk list ddan membuat dictionary movie_new.
"""

preparation = fix_movies #menyiapkan data
preparation.sort_values('movieId')

preparation = preparation.drop_duplicates('movieId') #melakukan drop pada data duplikat
preparation

#Konversi data series 'moveId' menjadi bentuk list
movie_id = preparation['movieId'].tolist()
movie_name = preparation['title'].tolist()
movie_tags = preparation['tags'].tolist()

print(len(movie_id))
print(len(movie_name))
print(len(movie_tags))

# Membuat dictionary untuk data 'movie_id', ‘movie_name’, dan 'tags

movie_new = pd.DataFrame({
    'id': movie_id,
    'name': movie_name,
    'tags': movie_tags
})

movie_new

"""# Modeling
Membuat model untuk dilakukan pelatihan dengan data yang sudah siap, disini terdapat 2 jenis algoritma model yaitu content based filtering dan collaborative filtering.

## Content Based Filtering
Algoritma ini bekerja dengan cara membandingkan kesamaan fitur antar content yang mana algoritma ini berfokus pada content untuk direkomendasikan kepada user yang belum menonton movie.
"""

data = movie_new #mengecek data
data.sample(5)

from sklearn.feature_extraction.text import TfidfVectorizer #menyiapkan model

tf = TfidfVectorizer()

tf.fit(data['tags'])

tf.get_feature_names_out()

tfidf_matrix = tf.fit_transform(data['tags']) #mengecek data dan mengubahnya ke dalam bentuk matriks

tfidf_matrix.shape

#mengubah tf-idf ke bentuk matriks
tfidf_matrix.todense()

#membuat dataframe

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.name
).sample(22, axis=1).sample(10, axis=0)

#cosine similarity

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

#melakukan pengecekan untuk kemiripan dari suatu film dari 0.0 hingga 1.0
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['name'], columns=data['name'])
print('Shape: ', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

def movie_recommendations(movie_name, similarity_data=cosine_sim_df, items=data[['name', 'tags']], k=5):
  index = similarity_data.loc[:, movie_name].to_numpy().argpartition(range(-1, -k, -1))

  #mengambil data dengan nilai similarity terbesar
  closest = similarity_data.columns[index[-1:-(k+2):-1]]

  # Drop nama_resto agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
  closest = closest.drop(movie_name, errors='ignore')

  return pd.DataFrame(closest).merge(items).head(k)

#Mencari data movie title toy story

data[data.name.eq('Dangerous Minds (1995)')]

#Mendapatkan rekomendasi untuk film yang mirip
movie_recommendations('Dangerous Minds (1995)')

"""## Collaborative Filtering"""

#import library
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

df = ratings #menyimpan ratings pada dataframe
df

"""# Data preparation
Untuk collaborative filtering dilakukan persiapan data lanjutan dimana dilakukan encoded untuk userId.

"""

#Kode ini mengambil daftar unik userId, lalu membuat mapping untuk mengonversi
#setiap userId menjadi indeks numerik (encoding).
#Selanjutnya, dibuat mapping balik untuk mengubah indeks numerik kembali ke userId aslinya (decoding).
#Ini berguna untuk merepresentasikan data dengan format yang lebih efisien, seperti dalam model machine learning
user_ids = df['userId'].unique().tolist()
print("List User ID: ", user_ids)

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print("encoded User ID: ", user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print("decoded User ID: ", user_encoded_to_user)

#melakukan encoded pada df movieId
movie_ids = df['movieId'].unique().tolist()

movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

df['user'] = df['userId'].map(user_to_user_encoded) #mengassign hasil encoded
df['movie'] = df['movieId'].map(movie_to_movie_encoded)

print(df.columns)
#meprint kolom pada df

#Kode ini menghitung jumlah pengguna dan film yang telah dienkode, mengonversi rating ke tipe float32,
#lalu menentukan serta mencetak jumlah pengguna, jumlah film, rating minimum, dan rating maksimum.
num_users = len(user_to_user_encoded)
print(num_users)

num_movie = len(movie_to_movie_encoded)
print(num_movie)

df['rating'] = df['rating'].values.astype(np.float32)

min_rating = min(df['rating'])
max_rating = max(df['rating'])

print("Number of Users: {}, Number of Movie: {}, Min Rating: {}, Max Rating: {}".format(num_users, num_movie, min_rating, max_rating))

"""## Data Splitting
Bagian ini membagi data menjadi data training dan data test
"""

#mengacak dataset
df = df.sample(frac=1, random_state=42)
df

#Kode ini mengekstrak fitur user dan movie sebagai array x, menormalisasi rating ke rentang 0-1 dalam array y, lalu membagi data menjadi 80% data latih dan 20% data validasi berdasarkan indeks.
x = df[['user', 'movie']].values

y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x,y)

# Pembuatan model Recommender System menggunakan TensorFlow dan Keras
class RecommenderNet(tf.keras.Model):
  def __init__(self, num_users, num_movie, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)

    # Inisialisasi jumlah pengguna, film, dan ukuran embedding
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size

    # Embedding layer untuk user dengan inisialisasi He dan regularisasi L2
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer='he_normal',
        embeddings_regularizer=keras.regularizers.l2(1e-6)
    )

    # Bias untuk user
    self.user_bias = layers.Embedding(num_users, 1)

    # Embedding layer untuk movie dengan inisialisasi He dan regularisasi L2
    self.movie_embedding = layers.Embedding(
        num_movie,
        embedding_size,
        embeddings_initializer='he_normal',
        embeddings_regularizer=keras.regularizers.l2(1e-6)
    )

    # Bias untuk movie
    self.resto_bias = layers.Embedding(num_movie, 1)

  def call(self, inputs):
    # Mengambil vektor embedding user dan bias user
    user_vector = self.user_embedding(inputs[:, 0])
    user_bias = self.user_bias(inputs[:, 0])

    # Mengambil vektor embedding movie dan bias movie
    movie_vector = self.movie_embedding(inputs[:, 1])
    movie_bias = self.resto_bias(inputs[:, 1])

    # Menghitung dot product antara vektor user dan movie
    dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)

    # Menambahkan bias user dan bias movie
    x = dot_user_movie + user_bias + movie_bias

    # Menggunakan fungsi aktivasi sigmoid
    return tf.nn.sigmoid(x)

#assign model dan melakukan compile
model = RecommenderNet(num_users, num_movie, 50)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics = [tf.keras.metrics.RootMeanSquaredError()]
)

"""## Training
Melakukan pelatihan pada model dengan data yang sudah disiapkan khusus.
"""

# Para baris ini dilakukan pelatihan atau fitting pada model dengan data training dan test
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

#grafik hasil pelatihan dan pengetesan
plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#melakukan pengecekan untuk movie yang belum ditonton oleh user
movie_df = movie_new
df = ratings

user_id = df.userId.sample(1).iloc[0]
movie_watched_by_user = df[df.userId == user_id]

movie_not_watched = movie_df[~movie_df['id'].isin(movie_watched_by_user.movieId.values)]['id']
movie_not_watched = list(
    set(movie_not_watched)
    .intersection(set(movie_to_movie_encoded.keys()))
)

movie_not_watched = [[movie_to_movie_encoded.get(x)] for x in movie_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_watched), movie_not_watched)
)

#melakukan pengetesan secara langsung pada model
ratings = model.predict(user_movie_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
reccomended_movie_ids = [
    movie_encoded_to_movie.get(movie_not_watched[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Movie with high ratings from user')
print('----' * 8)

top_movie_user = (
    movie_watched_by_user.sort_values(by='rating', ascending=False
    )
    .head(5)
    .movieId.values
)

movie_df_rows = movie_df[movie_df['id'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.name, ':', row.tags)

print('----' * 8)
print('Top 10 Movie recommendation')
print('----' * 8)

recommended_movie = movie_df[movie_df['id'].isin(reccomended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.name, ':', row.tags)